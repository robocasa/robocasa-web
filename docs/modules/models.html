
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Models &#8212; robomimic 0.3 documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../static/pygments.css" />
    <link rel="stylesheet" href="../static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../static/documentation_options.js"></script>
    <script src="../static/jquery.js"></script>
    <script src="../static/underscore.js"></script>
    <script src="../static/doctools.js"></script>
    <script src="../static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Configs" href="configs.html" />
    <link rel="prev" title="Algorithms" href="algorithms.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../static/robomimic_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">robomimic 0.3 documentation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/implemented_algorithms.html">
   Implemented Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/getting_started.html">
   Getting Started
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../datasets/overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../datasets/robomimic_v0.1.html">
   robomimic v0.1 (CoRL 2021)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../datasets/robosuite.html">
   robosuite Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../datasets/d4rl.html">
   D4RL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../datasets/momart.html">
   MOMART Datasets and Experiments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../datasets/roboturk_pilot.html">
   RoboTurk Pilot
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Pretrained Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../model_zoo/robomimic_v0.1.html">
   robomimic-v0.1
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/configs.html">
   Configuring and Launching Training Runs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/viewing_results.html">
   Logging and Viewing Training Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/hyperparam_scan.html">
   Running Hyperparameter Scans
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/reproducing_experiments.html">
   Reproducing Published Experiments and Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/dataset_contents.html">
   Dataset Contents and Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/training_transformers.html">
   Training Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/using_pretrained_models.html">
   Using Pretrained Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/observations.html">
   Multimodal Observations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/pretrained_representations.html">
   Pre-trained Visual Representations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/custom_algorithms.html">
   Implementing Custom Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorials/tensor_collections.html">
   Operations over Tensor Collections
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Modules
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dataset.html">
   SequenceDataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="algorithms.html">
   Algorithms
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="configs.html">
   Configs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="environments.html">
   Environments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Source API
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../api/robomimic.html">
   robomimic package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/robomimic.algo.html">
     robomimic.algo package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/robomimic.config.html">
     robomimic.config package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/robomimic.envs.html">
     robomimic.envs package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/robomimic.models.html">
     robomimic.models package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../api/robomimic.utils.html">
     robomimic.utils package
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Miscellaneous
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../miscellaneous/troubleshooting.html">
   Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../miscellaneous/contributing.html">
   Contributing Guidelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../miscellaneous/team.html">
   Team
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../miscellaneous/acknowledgments.html">
   Acknowledgments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../miscellaneous/references.html">
   Projects using robomimic
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Previous Versions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../versions/v0.2.html">
   v0.2
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../sources/modules/models.md.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#base-modules">
   Base Modules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#encodercore">
   EncoderCore
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualcore">
     VisualCore
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scancore">
     ScanCore
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#randomizers">
   Randomizers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#observation-encoder-and-decoder">
   Observation Encoder and Decoder
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-input-multi-output-mimo-modules">
   Multi-Input, Multi-Output (MIMO) Modules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implemented-policy-networks">
   Implemented Policy Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#actornetwork">
     ActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perturbationactornetwork">
     PerturbationActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussianactornetwork">
     GaussianActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gmmactornetwork">
     GMMActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rnnactornetwork">
     RNNActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rnngmmactornetwork">
     RNNGMMActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vaeactor">
     VAEActor
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implemented-value-networks">
   Implemented Value Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#valuenetwork">
     ValueNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributionalactionvaluenetwork">
     DistributionalActionValueNetwork
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implemented-vaes">
   Implemented VAEs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vae">
     VAE
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussianprior">
     GaussianPrior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categoricalprior">
     CategoricalPrior
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Models</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#base-modules">
   Base Modules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#encodercore">
   EncoderCore
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualcore">
     VisualCore
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scancore">
     ScanCore
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#randomizers">
   Randomizers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#observation-encoder-and-decoder">
   Observation Encoder and Decoder
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-input-multi-output-mimo-modules">
   Multi-Input, Multi-Output (MIMO) Modules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implemented-policy-networks">
   Implemented Policy Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#actornetwork">
     ActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perturbationactornetwork">
     PerturbationActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussianactornetwork">
     GaussianActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gmmactornetwork">
     GMMActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rnnactornetwork">
     RNNActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rnngmmactornetwork">
     RNNGMMActorNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vaeactor">
     VAEActor
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implemented-value-networks">
   Implemented Value Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#valuenetwork">
     ValueNetwork
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributionalactionvaluenetwork">
     DistributionalActionValueNetwork
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implemented-vaes">
   Implemented VAEs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vae">
     VAE
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussianprior">
     GaussianPrior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categoricalprior">
     CategoricalPrior
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">#</a></h1>
<p align="center">
  <img width="100.0%" src="../images/modules.png">
 </p><p><strong>robomimic</strong> implements a suite of reusable network modules at different abstraction levels that make creating new policy models easy.</p>
<section id="base-modules">
<h2>Base Modules<a class="headerlink" href="#base-modules" title="Permalink to this headline">#</a></h2>
<p>To support automated shape resolution when dealing with multiple modalities, all basic modules such as MLP and ConvNets defined in <code class="docutils literal notranslate"><span class="pre">robomimic.models.base_nets</span></code> are a subclass of <code class="docutils literal notranslate"><span class="pre">robomimic.models.base_nets.Module</span></code>, which requires implementing the abstract method <code class="docutils literal notranslate"><span class="pre">output_shape(self,</span> <span class="pre">input_shape=None)</span></code>. The function resolves the output shape (excluding the batch dimension) of the module either based on an external <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> or internal instance variables. To implement new base modules, simply inherit <code class="docutils literal notranslate"><span class="pre">robomimic.models.Module</span></code> or <code class="docutils literal notranslate"><span class="pre">robomimic.models.ConvBase</span></code> (if adding a ConvNet) and implement the abstract functions. Below is a sample implementation of a ResNet-18 base module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robomimic.models.base_nets</span> <span class="kn">import</span> <span class="n">ConvBase</span>

<span class="k">class</span> <span class="nc">ResNet18Conv</span><span class="p">(</span><span class="n">ConvBase</span><span class="p">):</span>

  <span class="o">...</span>

  <span class="k">def</span> <span class="nf">output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      Function to compute output shape from inputs to this module. </span>

<span class="sd">      Args:</span>
<span class="sd">          input_shape (iterable of int): shape of input. Does not include batch dimension.</span>
<span class="sd">              Some modules may not need this argument, if their output does not depend </span>
<span class="sd">              on the size of the input, or if they assume fixed size input.</span>

<span class="sd">      Returns:</span>
<span class="sd">          out_shape ([int]): list of integers corresponding to output shape</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span>
      <span class="n">out_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mf">32.</span><span class="p">))</span>
      <span class="n">out_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="mf">32.</span><span class="p">))</span>
      <span class="k">return</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="encodercore">
<h2>EncoderCore<a class="headerlink" href="#encodercore" title="Permalink to this headline">#</a></h2>
<p>We create the <code class="docutils literal notranslate"><span class="pre">EncoderCore</span></code> abstract class to encapsulate any network intended to encode a specific type of observation modality (e.g.: <code class="docutils literal notranslate"><span class="pre">VisualCore</span></code> for RGB and depth observations, and <code class="docutils literal notranslate"><span class="pre">ScanCore</span></code> for range scanner observations. See below for descriptions of both!). When a new encoder class is subclassed from <code class="docutils literal notranslate"><span class="pre">EncoderCore</span></code>, it will automatically be registered internally in robomimic, allowing the user to directly refer to their custom encoder classes in their config in string form. For example, if the user specifies a custom <code class="docutils literal notranslate"><span class="pre">EncoderCore</span></code>-based class named <code class="docutils literal notranslate"><span class="pre">MyCustomRGBEncoder</span></code> to encode RGB observations, they can directly set this in their config:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">observation</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">rgb</span><span class="o">.</span><span class="n">core_class</span> <span class="o">=</span> <span class="s2">&quot;MyCustomRGBEncoder&quot;</span>
<span class="n">config</span><span class="o">.</span><span class="n">observation</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">rgb</span><span class="o">.</span><span class="n">core_kwargs</span> <span class="o">=</span> <span class="o">...</span>
</pre></div>
</div>
<p>Any corresponding keyword arguments that should be passed to the encoder constructor should be specified in <code class="docutils literal notranslate"><span class="pre">core_kwargs</span></code> in the config. For more information on creating your own custom encoder, please see our <a class="reference external" href="../introduction/examples.html#custom-observation-modalities-example">example script</a>.</p>
<p>Below, we provide descriptions of specific EncoderCore-based classes used to encode RGB and depth observations (<code class="docutils literal notranslate"><span class="pre">VisualCore</span></code>) and range scanner observations (<code class="docutils literal notranslate"><span class="pre">ScanCore</span></code>).</p>
<section id="visualcore">
<h3>VisualCore<a class="headerlink" href="#visualcore" title="Permalink to this headline">#</a></h3>
<p>We provide a <code class="docutils literal notranslate"><span class="pre">VisualCore</span></code> module for constructing custom vision architectures. A <code class="docutils literal notranslate"><span class="pre">VisualCore</span></code> consists of a backbone network that featurizes image input — typically a <code class="docutils literal notranslate"><span class="pre">ConvBase</span></code> module — and a pooling module that reduces the feature tensor into a fixed-sized vector representation.  Below is a <code class="docutils literal notranslate"><span class="pre">VisualCore</span></code> built from a <code class="docutils literal notranslate"><span class="pre">ResNet18Conv</span></code> backbone and a <code class="docutils literal notranslate"><span class="pre">SpatialSoftmax</span></code> (<a class="reference external" href="https://rll.berkeley.edu/dsae/dsae.pdf">paper</a>) pooling module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robomimic.models.obs_core</span> <span class="kn">import</span> <span class="n">VisualCore</span>
<span class="kn">from</span> <span class="nn">robomimic.models.base_nets</span> <span class="kn">import</span> <span class="n">ResNet18Conv</span><span class="p">,</span> <span class="n">SpatialSoftmax</span>

<span class="n">vis_net</span> <span class="o">=</span> <span class="n">VisualCore</span><span class="p">(</span>
  <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span>
  <span class="n">core_class</span><span class="o">=</span><span class="s2">&quot;ResNet18Conv&quot;</span><span class="p">,</span>  <span class="c1"># use ResNet18 as the visualcore backbone</span>
  <span class="n">core_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;pretrained&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;input_coord_conv&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>  <span class="c1"># kwargs for the ResNet18Conv class</span>
  <span class="n">pool_class</span><span class="o">=</span><span class="s2">&quot;SpatialSoftmax&quot;</span><span class="p">,</span>  <span class="c1"># use spatial softmax to regularize the model output</span>
  <span class="n">pool_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_kp&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">},</span>  <span class="c1"># kwargs for the SpatialSoftmax --- use 32 keypoints</span>
  <span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># flatten the output of the spatial softmax layer</span>
  <span class="n">feature_dimension</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>  <span class="c1"># project the flattened feature into a 64-dim vector through a linear layer </span>
<span class="p">)</span>
</pre></div>
</div>
<p>New vision backbone and pooling classes can be added by subclassing <code class="docutils literal notranslate"><span class="pre">ConvBase</span></code>.</p>
</section>
<section id="scancore">
<h3>ScanCore<a class="headerlink" href="#scancore" title="Permalink to this headline">#</a></h3>
<p>We provide a <code class="docutils literal notranslate"><span class="pre">ScanCore</span></code> module for constructing custom range finder architectures. <code class="docutils literal notranslate"><span class="pre">ScanCore</span></code> consists of a 1D Convolution backbone network (<code class="docutils literal notranslate"><span class="pre">Conv1dBase</span></code>) that featurizes a high-dimensional 1D input, and a pooling module that reduces the feature tensor into a fixed-sized vector representation.  Below is an example of a <code class="docutils literal notranslate"><span class="pre">ScanCore</span></code> network with a <code class="docutils literal notranslate"><span class="pre">SpatialSoftmax</span></code> (<a class="reference external" href="https://rll.berkeley.edu/dsae/dsae.pdf">paper</a>) pooling module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robomimic.models.obs_core</span> <span class="kn">import</span> <span class="n">ScanCore</span>
<span class="kn">from</span> <span class="nn">robomimic.models.base_nets</span> <span class="kn">import</span> <span class="n">SpatialSoftmax</span>

<span class="n">vis_net</span> <span class="o">=</span> <span class="n">ScanCore</span><span class="p">(</span>
  <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">120</span><span class="p">),</span>
  <span class="n">conv_kwargs</span><span class="o">=</span><span class="p">{</span>
      <span class="s2">&quot;out_channels&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>
      <span class="s2">&quot;kernel_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
      <span class="s2">&quot;stride&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
  <span class="p">},</span>    <span class="c1"># kwarg settings to pass to individual Conv1d layers</span>
  <span class="n">conv_activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>   <span class="c1"># use relu in between each Conv1d layer</span>
  <span class="n">pool_class</span><span class="o">=</span><span class="s2">&quot;SpatialSoftmax&quot;</span><span class="p">,</span>  <span class="c1"># use spatial softmax to regularize the model output</span>
  <span class="n">pool_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_kp&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">},</span>  <span class="c1"># kwargs for the SpatialSoftmax --- use 32 keypoints</span>
  <span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># flatten the output of the spatial softmax layer</span>
  <span class="n">feature_dimension</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>  <span class="c1"># project the flattened feature into a 64-dim vector through a linear layer </span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="randomizers">
<h2>Randomizers<a class="headerlink" href="#randomizers" title="Permalink to this headline">#</a></h2>
<p>Randomizers are <code class="docutils literal notranslate"><span class="pre">Modules</span></code> that perturb network inputs during training, and optionally during evaluation. A <code class="docutils literal notranslate"><span class="pre">Randomizer</span></code> implements a <code class="docutils literal notranslate"><span class="pre">forward_in</span></code> and a <code class="docutils literal notranslate"><span class="pre">forward_out</span></code> function, which are intended to process the input and output of a neural network module. As an example, the <code class="docutils literal notranslate"><span class="pre">forward_in</span></code> function of a <code class="docutils literal notranslate"><span class="pre">CropRandomizer</span></code> instance perturbs an input image by taking a random crop of the image (as shown in th gif below). If the <code class="docutils literal notranslate"><span class="pre">CropRandomizer</span></code> is configured to take more than one random crop (<code class="docutils literal notranslate"><span class="pre">n_crops</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>) of each input image, it will send all random crops through the image network and reduce the network output by average pooling the outputs along the <code class="docutils literal notranslate"><span class="pre">n_crops</span></code> dimension in the <code class="docutils literal notranslate"><span class="pre">forward_out</span></code> function.</p>
<img src="../images/randomizer.gif" alt="Image illustration of a CropRandomizer" style="zoom:50%;" /><p><code class="docutils literal notranslate"><span class="pre">Randomizer</span></code> modules are intended to be used alongside an <code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> — see the next section for more details. Additional randomizer classes can be implemented by subclassing the <code class="docutils literal notranslate"><span class="pre">Randomizer</span></code> class and implementing the necessary abstract functions.</p>
<p><strong>Visualizing randomized input:</strong> To visualize the original and randomized image input, set <code class="docutils literal notranslate"><span class="pre">VISUALIZE_RANDOMIZER</span> <span class="pre">=</span> <span class="pre">True</span></code> in <code class="docutils literal notranslate"><span class="pre">robomimic/macros.py</span></code></p>
</section>
<section id="observation-encoder-and-decoder">
<h2>Observation Encoder and Decoder<a class="headerlink" href="#observation-encoder-and-decoder" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> and <code class="docutils literal notranslate"><span class="pre">ObservationDecoder</span></code> are basic building blocks for dealing with observation dictionary inputs and outputs. They are designed to take in multiple streams of observation modalities as input (e.g. a dictionary containing images and robot proprioception signals), and output a dictionary of predictions like actions and subgoals. Below is an example of how to manually create an <code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> instance by registering observation modalities with the <code class="docutils literal notranslate"><span class="pre">register_obs_key</span></code> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robomimic.models.base_nets</span> <span class="kn">import</span> <span class="n">MLP</span>
<span class="kn">from</span> <span class="nn">robomimic.models.obs_core</span> <span class="kn">import</span> <span class="n">VisualCore</span><span class="p">,</span> <span class="n">CropRandomizer</span>
<span class="kn">from</span> <span class="nn">robomimic.models.obs_nets</span> <span class="kn">import</span> <span class="n">ObservationEncoder</span><span class="p">,</span> <span class="n">ObservationDecoder</span>

<span class="n">obs_encoder</span> <span class="o">=</span> <span class="n">ObservationEncoder</span><span class="p">(</span><span class="n">feature_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">)</span>

<span class="c1"># There are two ways to construct the network for processing a input modality.</span>
<span class="c1"># Assume we are processing image input of shape (3, 224, 224).</span>
<span class="n">camera1_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">]</span>

<span class="n">image_randomizer</span> <span class="o">=</span> <span class="n">CropRandomizer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">camera2_shape</span><span class="p">,</span> <span class="n">crop_height</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">crop_width</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="c1"># We will use a reconfigurable image processing backbone VisualCore to process the input image modality</span>
<span class="n">net_class</span> <span class="o">=</span> <span class="s2">&quot;VisualCore&quot;</span>  <span class="c1"># this is defined in models/base_nets.py</span>

<span class="c1"># kwargs for VisualCore network</span>
<span class="n">net_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;input_shape&quot;</span><span class="p">:</span> <span class="n">camera1_shape</span><span class="p">,</span>
    <span class="s2">&quot;core_class&quot;</span><span class="p">:</span> <span class="s2">&quot;ResNet18Conv&quot;</span><span class="p">,</span>  <span class="c1"># use ResNet18 as the visualcore backbone</span>
    <span class="s2">&quot;core_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;pretrained&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;input_coord_conv&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s2">&quot;pool_class&quot;</span><span class="p">:</span> <span class="s2">&quot;SpatialSoftmax&quot;</span><span class="p">,</span>  <span class="c1"># use spatial softmax to regularize the model output</span>
    <span class="s2">&quot;pool_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;num_kp&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">}</span>
<span class="p">}</span>

<span class="c1"># register the network for processing the modality</span>
<span class="n">obs_encoder</span><span class="o">.</span><span class="n">register_obs_key</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;camera1&quot;</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="n">camera1_shape</span><span class="p">,</span>
    <span class="n">net_class</span><span class="o">=</span><span class="n">net_class</span><span class="p">,</span>
    <span class="n">net_kwargs</span><span class="o">=</span><span class="n">net_kwargs</span><span class="p">,</span>
    <span class="n">randomizer</span><span class="o">=</span><span class="n">image_randomizer</span>
<span class="p">)</span>

<span class="c1"># We could mix low-dimensional observation, e.g., proprioception signal, in the encoder</span>
<span class="n">proprio_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">]</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">layer_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,),</span> <span class="n">output_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">obs_encoder</span><span class="o">.</span><span class="n">register_obs_key</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;proprio&quot;</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="n">proprio_shape</span><span class="p">,</span>
    <span class="n">net</span><span class="o">=</span><span class="n">net</span>
<span class="p">)</span>
</pre></div>
</div>
<p>By default, each modality network should reduce an input observation stream to a fixed-size vector. The output of the <code class="docutils literal notranslate"><span class="pre">forward</span></code> function of the <code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> is simply the concatenation of all the vectors. The order of concatenation is deterministic and is the the same as the order that the modalities are registered. <code class="docutils literal notranslate"><span class="pre">ObservationGroupEncoder</span></code> further supports encoding nested groups of observations, e.g., <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">goal</span></code>, and <code class="docutils literal notranslate"><span class="pre">subgoal</span></code>. This allows constructing goal-conditioned and / or subgoal-conditioned policy models.</p>
<p>However, it can be tedious to enumerate all modalities when creating a policy model. The standard entry point to create an <code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> is via the <code class="docutils literal notranslate"><span class="pre">observation_encoder_factory</span></code> function in <code class="docutils literal notranslate"><span class="pre">robomimic.models.obs_nets</span></code>. It will enumerate all observation modalities and initialize all modality networks according to the configurations provided by the <code class="docutils literal notranslate"><span class="pre">config.observation</span></code> config section.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ObservationDecoder</span></code> class is relatively straightforward. It’s simply a single-input, multi-output-head MLP. For example, the following snippet creates an <code class="docutils literal notranslate"><span class="pre">ObservationDecoder</span></code> that takes the output of the observation encoder as input and outputs two action predictions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">obs_decoder</span> <span class="o">=</span> <span class="n">ObservationDecoder</span><span class="p">(</span>
    <span class="n">input_feat_dim</span><span class="o">=</span><span class="n">obs_encoder</span><span class="o">.</span><span class="n">output_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">decode_shapes</span><span class="o">=</span><span class="n">OrderedDict</span><span class="p">({</span><span class="s2">&quot;action_pos&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="s2">&quot;action_orn&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,)})</span>
<span class="p">)</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">examples/simple_obs_nets.py</span></code> for the complete example that shows additional functionalities such as weight sharing among modality networks.</p>
</section>
<section id="multi-input-multi-output-mimo-modules">
<h2>Multi-Input, Multi-Output (MIMO) Modules<a class="headerlink" href="#multi-input-multi-output-mimo-modules" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">MIMO_MLP</span></code> and <code class="docutils literal notranslate"><span class="pre">RNN_MIMO_MLP</span></code> are highest-level wrappers that use <code class="docutils literal notranslate"><span class="pre">ObservationGroupEncoder</span></code> and <code class="docutils literal notranslate"><span class="pre">ObservationDecoder</span></code> to create multi-input, multi-output network architectures. <code class="docutils literal notranslate"><span class="pre">MIMO_MLP</span></code> optionally adds additional MLP layers before piping the encoded feature vector to the decoder.</p>
<p><code class="docutils literal notranslate"><span class="pre">RNN_MIMO_MLP</span></code> encodes each observation in an observation sequence using <code class="docutils literal notranslate"><span class="pre">ObservationGroupEncoder</span></code> and digests the feature sequence using RNN-variants such as LSTM and GRU networks.</p>
<p><code class="docutils literal notranslate"><span class="pre">MIMO_MLP</span></code> and <code class="docutils literal notranslate"><span class="pre">RNN_MIMO_MLP</span></code> serve as <strong>the backbone for all policy and value network models</strong> – these models simple subclass a <code class="docutils literal notranslate"><span class="pre">MIMO_MLP</span></code> or <code class="docutils literal notranslate"><span class="pre">RNN_MIMO_MLP</span></code>, and adds specific input and output shapes.</p>
</section>
<section id="implemented-policy-networks">
<h2>Implemented Policy Networks<a class="headerlink" href="#implemented-policy-networks" title="Permalink to this headline">#</a></h2>
<p>These networks take an observation dictionary as input (and possibly additional conditioning, such as subgoal or goal dictionaries) and produce action predictions, samples, or distributions as outputs. Note that actions are assumed to lie in <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>, and most networks will have a final <code class="docutils literal notranslate"><span class="pre">tanh</span></code> activation to help ensure that outputs lie within this range. See <code class="docutils literal notranslate"><span class="pre">robomimic/models/policy_nets.py</span></code> for complete implementations.</p>
<section id="actornetwork">
<h3>ActorNetwork<a class="headerlink" href="#actornetwork" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A basic policy network that predicts actions from observations. Can optionally be goal conditioned on future observations.</p></li>
</ul>
</section>
<section id="perturbationactornetwork">
<h3>PerturbationActorNetwork<a class="headerlink" href="#perturbationactornetwork" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>An action perturbation network - primarily used in BCQ. It takes states and actions and returns action perturbations.</p></li>
</ul>
</section>
<section id="gaussianactornetwork">
<h3>GaussianActorNetwork<a class="headerlink" href="#gaussianactornetwork" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Variant of actor network that outputs a diagonal unimodal Gaussian distribution as action predictions.</p></li>
</ul>
</section>
<section id="gmmactornetwork">
<h3>GMMActorNetwork<a class="headerlink" href="#gmmactornetwork" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Variant of actor network that outputs a multimodal Gaussian mixture distribution as action predictions.</p></li>
</ul>
</section>
<section id="rnnactornetwork">
<h3>RNNActorNetwork<a class="headerlink" href="#rnnactornetwork" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>An RNN policy network that predicts actions from a sequence of observations.</p></li>
</ul>
</section>
<section id="rnngmmactornetwork">
<h3>RNNGMMActorNetwork<a class="headerlink" href="#rnngmmactornetwork" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>An RNN policy network that outputs a multimodal Gaussian mixture distribution over actions from a sequence of observations.</p></li>
</ul>
</section>
<section id="vaeactor">
<h3>VAEActor<a class="headerlink" href="#vaeactor" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A VAE that models a distribution of actions conditioned on observations. The VAE prior and decoder are used at test-time as the policy.</p></li>
</ul>
</section>
</section>
<section id="implemented-value-networks">
<h2>Implemented Value Networks<a class="headerlink" href="#implemented-value-networks" title="Permalink to this headline">#</a></h2>
<p>These networks take an observation dictionary as input (and possibly additional conditioning, such as subgoal or goal dictionaries) and produce value or action-value estimates or distributions. See <code class="docutils literal notranslate"><span class="pre">robomimic/models/value_nets.py</span></code> for complete implementations.</p>
<section id="valuenetwork">
<h3>ValueNetwork<a class="headerlink" href="#valuenetwork" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A basic value network that predicts values from observations. Can optionally be goal conditioned on future observations.</p></li>
</ul>
</section>
<section id="distributionalactionvaluenetwork">
<h3>DistributionalActionValueNetwork<a class="headerlink" href="#distributionalactionvaluenetwork" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Distributional Q (action-value) network that outputs a categorical distribution over a discrete grid of value atoms. See the <a class="reference external" href="https://arxiv.org/abs/1707.06887">paper</a> for more details.</p></li>
</ul>
</section>
</section>
<section id="implemented-vaes">
<h2>Implemented VAEs<a class="headerlink" href="#implemented-vaes" title="Permalink to this headline">#</a></h2>
<p>The library implements a general VAE architecture and a number of prior distributions. See <code class="docutils literal notranslate"><span class="pre">robomimic/models/vae_nets.py</span></code> for complete implementations.</p>
<section id="vae">
<h3>VAE<a class="headerlink" href="#vae" title="Permalink to this headline">#</a></h3>
<p>A general Variational Autoencoder (VAE) implementation, as described in this <a class="reference external" href="https://arxiv.org/abs/1312.6114">paper</a>.</p>
<p>Models a distribution p(X) or a conditional distribution p(X | Y), where each variable can consist of multiple modalities. The target variable X, whose distribution is modeled, is specified through the <code class="docutils literal notranslate"><span class="pre">input_shapes</span></code> argument, which is a map between modalities (strings) and expected shapes. In this way, a variable that consists of multiple kinds of data (e.g. image and flat-dimensional) can be modeled as well. A separate <code class="docutils literal notranslate"><span class="pre">output_shapes</span></code> argument is used to specify the expected reconstructions - this allows for asymmetric reconstruction (for example, reconstructing low-resolution images).</p>
<p>This implementation supports learning conditional distributions as well (cVAE). The conditioning variable Y is specified through the <code class="docutils literal notranslate"><span class="pre">condition_shapes</span></code> argument, which is also a map between modalities (strings) and expected shapes. In this way, variables with multiple kinds of data (e.g. image and flat-dimensional) can jointly be conditioned on. By default, the decoder takes the conditioning variable Y as input. To force the decoder to reconstruct from just the latent, set <code class="docutils literal notranslate"><span class="pre">decoder_is_conditioned</span></code> to False (in this case, the prior must be conditioned).</p>
<p>The implementation also supports learning expressive priors instead of using the usual N(0, 1) prior. There are three kinds of priors supported - Gaussian, Gaussian Mixture Model (GMM), and Categorical. For each prior, the parameters can be learned as independent parameters, or be learned as functions of the conditioning variable Y (by setting <code class="docutils literal notranslate"><span class="pre">prior_is_conditioned</span></code>).</p>
</section>
<section id="gaussianprior">
<h3>GaussianPrior<a class="headerlink" href="#gaussianprior" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A class that holds functionality for learning both unimodal Gaussian priors and multimodal Gaussian Mixture Model priors for use in VAEs. Supports learnable priors, learnable / fixed mixture weights for GMM, and observation-conditioned priors,</p></li>
</ul>
</section>
<section id="categoricalprior">
<h3>CategoricalPrior<a class="headerlink" href="#categoricalprior" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A class that holds functionality for learning categorical priors for use in VAEs.</p></li>
</ul>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="algorithms.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Algorithms</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="configs.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Configs</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By the robomimic core team<br/>
  
      &copy; Copyright the robomimic core team, 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>