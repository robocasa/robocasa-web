

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Models &mdash; robomimic 0.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../static/theme_overrides.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
        <script src="../static/jquery.js"></script>
        <script src="../static/underscore.js"></script>
        <script src="../static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Configs" href="configs.html" />
    <link rel="prev" title="Algorithms" href="algorithms.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> robomimic
          

          
          </a>

          
            
            
              <div class="version">
                0.2.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/quickstart.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/advanced.html">Advanced Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/examples.html">Working with robomimic Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/datasets.html">Using Demonstration Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/model_zoo.html">Using the Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/results.html">Reproducing Study Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/model_zoo.html">Using the Model Zoo</a></li>
</ul>
<p class="caption"><span class="caption-text">Modules</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataset.html">SequenceDataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="observations.html">Observations</a></li>
<li class="toctree-l1"><a class="reference internal" href="algorithms.html">Algorithms</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#base-modules">Base Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#visualcore">VisualCore</a></li>
<li class="toctree-l2"><a class="reference internal" href="#randomizers">Randomizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#observation-encoder-and-decoder">Observation Encoder and Decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multi-input-multi-output-mimo-modules">Multi-Input, Multi-Output (MIMO) Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#implemented-policy-networks">Implemented Policy Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#actornetwork">ActorNetwork</a></li>
<li class="toctree-l3"><a class="reference internal" href="#perturbationactornetwork">PerturbationActorNetwork</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gaussianactornetwork">GaussianActorNetwork</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gmmactornetwork">GMMActorNetwork</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnnactornetwork">RNNActorNetwork</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnngmmactornetwork">RNNGMMActorNetwork</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vaeactor">VAEActor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#implemented-value-networks">Implemented Value Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#valuenetwork">ValueNetwork</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distributionalactionvaluenetwork">DistributionalActionValueNetwork</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#implemented-vaes">Implemented VAEs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vae">VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gaussianprior">GaussianPrior</a></li>
<li class="toctree-l3"><a class="reference internal" href="#categoricalprior">CategoricalPrior</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="configs.html">Configs</a></li>
<li class="toctree-l1"><a class="reference internal" href="environments.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Source API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/robomimic.html">robomimic package</a></li>
</ul>
<p class="caption"><span class="caption-text">Miscellaneous</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../miscellaneous/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../miscellaneous/contributing.html">Contributing Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../miscellaneous/team.html">Team</a></li>
<li class="toctree-l1"><a class="reference internal" href="../miscellaneous/acknowledgments.html">Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../miscellaneous/references.html">Projects using robomimic</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">robomimic</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Models</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../sources/modules/models.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h1>
<p><img alt="overview" src="../images/modules.png" /></p>
<p><strong>robomimic</strong> implements a suite of reusable network modules at different abstraction levels that make creating new policy models easy.</p>
<div class="section" id="base-modules">
<h2>Base Modules<a class="headerlink" href="#base-modules" title="Permalink to this headline">¶</a></h2>
<p>To support automated shape resolution when dealing with multiple modalities, all basic modules such as MLP and ConvNets defined in <code class="docutils literal notranslate"><span class="pre">robomimic.models.base_nets</span></code> are a subclass of <code class="docutils literal notranslate"><span class="pre">robomimic.models.base_nets.Module</span></code>, which requires implementing the abstract method <code class="docutils literal notranslate"><span class="pre">output_shape(self,</span> <span class="pre">input_shape=None)</span></code>. The function resolves the output shape (excluding the batch dimension) of the module either based on an external <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> or internal instance variables. To implement new base modules, simply inherit <code class="docutils literal notranslate"><span class="pre">robomimic.models.Module</span></code> or <code class="docutils literal notranslate"><span class="pre">robomimic.models.ConvBase</span></code> (if adding a ConvNet) and implement the abstract functions. Below is a sample implementation of a ResNet-18 base module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robomimic.models.base_nets</span> <span class="kn">import</span> <span class="n">ConvBase</span>

<span class="k">class</span> <span class="nc">ResNet18Conv</span><span class="p">(</span><span class="n">ConvBase</span><span class="p">):</span>

  <span class="o">...</span>

  <span class="k">def</span> <span class="nf">output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      Function to compute output shape from inputs to this module. </span>

<span class="sd">      Args:</span>
<span class="sd">          input_shape (iterable of int): shape of input. Does not include batch dimension.</span>
<span class="sd">              Some modules may not need this argument, if their output does not depend </span>
<span class="sd">              on the size of the input, or if they assume fixed size input.</span>

<span class="sd">      Returns:</span>
<span class="sd">          out_shape ([int]): list of integers corresponding to output shape</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span>
      <span class="n">out_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="mf">32.</span><span class="p">))</span>
      <span class="n">out_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="mf">32.</span><span class="p">))</span>
      <span class="k">return</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="visualcore">
<h2>VisualCore<a class="headerlink" href="#visualcore" title="Permalink to this headline">¶</a></h2>
<p>We provide a <code class="docutils literal notranslate"><span class="pre">VisualCore</span></code> module for constructing custom vision architectures. A <code class="docutils literal notranslate"><span class="pre">VisualCore</span></code> consists of a backbone network that featurizes image input — typically a <code class="docutils literal notranslate"><span class="pre">ConvBase</span></code> module — and a pooling module that reduces the feature tensor into a fixed-sized vector representation.  Below is a <code class="docutils literal notranslate"><span class="pre">VisualCore</span></code> built from a <code class="docutils literal notranslate"><span class="pre">ResNet18Conv</span></code> backbone and a <code class="docutils literal notranslate"><span class="pre">SpatialSoftmax</span></code> (<a class="reference external" href="https://rll.berkeley.edu/dsae/dsae.pdf">paper</a>) pooling module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robomimic.models.base_nets</span> <span class="kn">import</span> <span class="n">VisualCore</span><span class="p">,</span> <span class="n">ResNet18Conv</span><span class="p">,</span> <span class="n">SpatialSoftmax</span>

<span class="n">vis_net</span> <span class="o">=</span> <span class="n">VisualCore</span><span class="p">(</span>
  <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span>
  <span class="n">core_class</span><span class="o">=</span><span class="s2">&quot;ResNet18Conv&quot;</span><span class="p">,</span>  <span class="c1"># use ResNet18 as the visualcore backbone</span>
  <span class="n">core_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;pretrained&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;input_coord_conv&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>  <span class="c1"># kwargs for the ResNet18Conv class</span>
  <span class="n">pool_class</span><span class="o">=</span><span class="s2">&quot;SpatialSoftmax&quot;</span><span class="p">,</span>  <span class="c1"># use spatial softmax to regularize the model output</span>
  <span class="n">pool_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_kp&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">},</span>  <span class="c1"># kwargs for the SpatialSoftmax --- use 32 keypoints</span>
  <span class="n">flatten</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># flatten the output of the spatial softmax layer</span>
  <span class="n">feature_dimension</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>  <span class="c1"># project the flattened feature into a 64-dim vector through a linear layer </span>
<span class="p">)</span>
</pre></div>
</div>
<p>New vision backbone and pooling classes can be added by subclassing <code class="docutils literal notranslate"><span class="pre">ConvBase</span></code>.</p>
</div>
<div class="section" id="randomizers">
<h2>Randomizers<a class="headerlink" href="#randomizers" title="Permalink to this headline">¶</a></h2>
<p>Randomizers are <code class="docutils literal notranslate"><span class="pre">Modules</span></code> that perturb network inputs during training, and optionally during evaluation. A <code class="docutils literal notranslate"><span class="pre">Randomizer</span></code> implements a <code class="docutils literal notranslate"><span class="pre">forward_in</span></code> and a <code class="docutils literal notranslate"><span class="pre">forward_out</span></code> function, which are intended to process the input and output of a neural network module. As an example, the <code class="docutils literal notranslate"><span class="pre">forward_in</span></code> function of a <code class="docutils literal notranslate"><span class="pre">CropRandomizer</span></code> instance perturbs an input image by taking a random crop of the image (as shown in th gif below). If the <code class="docutils literal notranslate"><span class="pre">CropRandomizer</span></code> is configured to take more than one random crop (<code class="docutils literal notranslate"><span class="pre">n_crops</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>) of each input image, it will send all random crops through the image network and reduce the network output by average pooling the outputs along the <code class="docutils literal notranslate"><span class="pre">n_crops</span></code> dimension in the <code class="docutils literal notranslate"><span class="pre">forward_out</span></code> function.</p>
<img src="../images/randomizer.gif" alt="Image illustration of a CropRandomizer" style="zoom:50%;" /><p><code class="docutils literal notranslate"><span class="pre">Randomizer</span></code> modules are intended to be used alongside an <code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> — see the next section for more details. Additional randomizer classes can be implemented by subclassing the <code class="docutils literal notranslate"><span class="pre">Randomizer</span></code> class and implementing the necessary abstract functions.</p>
</div>
<div class="section" id="observation-encoder-and-decoder">
<h2>Observation Encoder and Decoder<a class="headerlink" href="#observation-encoder-and-decoder" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> and <code class="docutils literal notranslate"><span class="pre">ObservationDecoder</span></code> are basic building blocks for dealing with observation dictionary inputs and outputs. They are designed to take in multiple streams of observation modalities as input (e.g. a dictionary containing images and robot proprioception signals), and output a dictionary of predictions like actions and subgoals. Below is an example of how to manually create an <code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> instance by registering observation modalities with the <code class="docutils literal notranslate"><span class="pre">register_obs_key</span></code> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">robomimic.models.obs_nets</span> <span class="kn">import</span> <span class="n">ObservationEncoder</span><span class="p">,</span> <span class="n">CropRandomizer</span><span class="p">,</span> <span class="n">MLP</span><span class="p">,</span> <span class="n">VisualCore</span><span class="p">,</span> <span class="n">ObservationDecoder</span>

<span class="n">obs_encoder</span> <span class="o">=</span> <span class="n">ObservationEncoder</span><span class="p">(</span><span class="n">feature_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">)</span>

<span class="c1"># There are two ways to construct the network for processing a input modality.</span>
<span class="c1"># Assume we are processing image input of shape (3, 224, 224).</span>
<span class="n">camera1_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">]</span>

<span class="n">image_randomizer</span> <span class="o">=</span> <span class="n">CropRandomizer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">camera2_shape</span><span class="p">,</span> <span class="n">crop_height</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">crop_width</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="c1"># We will use a reconfigurable image processing backbone VisualCore to process the input image modality</span>
<span class="n">net_class</span> <span class="o">=</span> <span class="s2">&quot;VisualCore&quot;</span>  <span class="c1"># this is defined in models/base_nets.py</span>

<span class="c1"># kwargs for VisualCore network</span>
<span class="n">net_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;input_shape&quot;</span><span class="p">:</span> <span class="n">camera1_shape</span><span class="p">,</span>
    <span class="s2">&quot;core_class&quot;</span><span class="p">:</span> <span class="s2">&quot;ResNet18Conv&quot;</span><span class="p">,</span>  <span class="c1"># use ResNet18 as the visualcore backbone</span>
    <span class="s2">&quot;core_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;pretrained&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;input_coord_conv&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s2">&quot;pool_class&quot;</span><span class="p">:</span> <span class="s2">&quot;SpatialSoftmax&quot;</span><span class="p">,</span>  <span class="c1"># use spatial softmax to regularize the model output</span>
    <span class="s2">&quot;pool_kwargs&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;num_kp&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">}</span>
<span class="p">}</span>

<span class="c1"># register the network for processing the modality</span>
<span class="n">obs_encoder</span><span class="o">.</span><span class="n">register_obs_key</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;camera1&quot;</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="n">camera1_shape</span><span class="p">,</span>
    <span class="n">net_class</span><span class="o">=</span><span class="n">net_class</span><span class="p">,</span>
    <span class="n">net_kwargs</span><span class="o">=</span><span class="n">net_kwargs</span><span class="p">,</span>
    <span class="n">randomizer</span><span class="o">=</span><span class="n">image_randomizer</span>
<span class="p">)</span>

<span class="c1"># We could mix low-dimensional observation, e.g., proprioception signal, in the encoder</span>
<span class="n">proprio_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">]</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">layer_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,),</span> <span class="n">output_activation</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">obs_encoder</span><span class="o">.</span><span class="n">register_obs_key</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;proprio&quot;</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="n">proprio_shape</span><span class="p">,</span>
    <span class="n">net</span><span class="o">=</span><span class="n">net</span>
<span class="p">)</span>
</pre></div>
</div>
<p>By default, each modality network should reduce an input observation stream to a fixed-size vector. The output of the <code class="docutils literal notranslate"><span class="pre">forward</span></code> function of the <code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> is simply the concatenation of all the vectors. The order of concatenation is deterministic and is the the same as the order that the modalities are registered. <code class="docutils literal notranslate"><span class="pre">ObservationGroupEncoder</span></code> further supports encoding nested groups of observations, e.g., <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">goal</span></code>, and <code class="docutils literal notranslate"><span class="pre">subgoal</span></code>. This allows constructing goal-conditioned and / or subgoal-conditioned policy models.</p>
<p>However, it can be tedious to enumerate all modalities when creating a policy model. The standard entry point to create an <code class="docutils literal notranslate"><span class="pre">ObservationEncoder</span></code> is via the <code class="docutils literal notranslate"><span class="pre">observation_encoder_factory</span></code> function in <code class="docutils literal notranslate"><span class="pre">robomimic.models.obs_nets</span></code>. It will enumerate all observation modalities and initialize all modality networks according to the configurations provided by the <code class="docutils literal notranslate"><span class="pre">config.observation</span></code> config section.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ObservationDecoder</span></code> class is relatively straightforward. It’s simply a single-input, multi-output-head MLP. For example, the following snippet creates an <code class="docutils literal notranslate"><span class="pre">ObservationDecoder</span></code> that takes the output of the observation encoder as input and outputs two action predictions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">obs_decoder</span> <span class="o">=</span> <span class="n">ObservationDecoder</span><span class="p">(</span>
    <span class="n">input_feat_dim</span><span class="o">=</span><span class="n">obs_encoder</span><span class="o">.</span><span class="n">output_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">decode_shapes</span><span class="o">=</span><span class="n">OrderedDict</span><span class="p">({</span><span class="s2">&quot;action_pos&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="s2">&quot;action_orn&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,)})</span>
<span class="p">)</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">examples/simple_obs_nets.py</span></code> for the complete example that shows additional functionalities such as weight sharing among modality networks.</p>
</div>
<div class="section" id="multi-input-multi-output-mimo-modules">
<h2>Multi-Input, Multi-Output (MIMO) Modules<a class="headerlink" href="#multi-input-multi-output-mimo-modules" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">MIMO_MLP</span></code> and <code class="docutils literal notranslate"><span class="pre">RNN_MIMO_MLP</span></code> are highest-level wrappers that use <code class="docutils literal notranslate"><span class="pre">ObservationGroupEncoder</span></code> and <code class="docutils literal notranslate"><span class="pre">ObservationDecoder</span></code> to create multi-input, multi-output network architectures. <code class="docutils literal notranslate"><span class="pre">MIMO_MLP</span></code> optionally adds additional MLP layers before piping the encoded feature vector to the decoder.</p>
<p><code class="docutils literal notranslate"><span class="pre">RNN_MIMO_MLP</span></code> encodes each observation in an observation sequence using <code class="docutils literal notranslate"><span class="pre">ObservationGroupEncoder</span></code> and digests the feature sequence using RNN-variants such as LSTM and GRU networks.</p>
<p><code class="docutils literal notranslate"><span class="pre">MIMO_MLP</span></code> and <code class="docutils literal notranslate"><span class="pre">RNN_MIMO_MLP</span></code> serve as <strong>the backbone for all policy and value network models</strong> – these models simple subclass a <code class="docutils literal notranslate"><span class="pre">MIMO_MLP</span></code> or <code class="docutils literal notranslate"><span class="pre">RNN_MIMO_MLP</span></code>, and adds specific input and output shapes.</p>
</div>
<div class="section" id="implemented-policy-networks">
<h2>Implemented Policy Networks<a class="headerlink" href="#implemented-policy-networks" title="Permalink to this headline">¶</a></h2>
<p>These networks take an observation dictionary as input (and possibly additional conditioning, such as subgoal or goal dictionaries) and produce action predictions, samples, or distributions as outputs. Note that actions are assumed to lie in <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>, and most networks will have a final <code class="docutils literal notranslate"><span class="pre">tanh</span></code> activation to help ensure that outputs lie within this range. See <code class="docutils literal notranslate"><span class="pre">robomimic/models/policy_nets.py</span></code> for complete implementations.</p>
<div class="section" id="actornetwork">
<h3>ActorNetwork<a class="headerlink" href="#actornetwork" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A basic policy network that predicts actions from observations. Can optionally be goal conditioned on future observations.</p></li>
</ul>
</div>
<div class="section" id="perturbationactornetwork">
<h3>PerturbationActorNetwork<a class="headerlink" href="#perturbationactornetwork" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>An action perturbation network - primarily used in BCQ. It takes states and actions and returns action perturbations.</p></li>
</ul>
</div>
<div class="section" id="gaussianactornetwork">
<h3>GaussianActorNetwork<a class="headerlink" href="#gaussianactornetwork" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Variant of actor network that outputs a diagonal unimodal Gaussian distribution as action predictions.</p></li>
</ul>
</div>
<div class="section" id="gmmactornetwork">
<h3>GMMActorNetwork<a class="headerlink" href="#gmmactornetwork" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Variant of actor network that outputs a multimodal Gaussian mixture distribution as action predictions.</p></li>
</ul>
</div>
<div class="section" id="rnnactornetwork">
<h3>RNNActorNetwork<a class="headerlink" href="#rnnactornetwork" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>An RNN policy network that predicts actions from a sequence of observations.</p></li>
</ul>
</div>
<div class="section" id="rnngmmactornetwork">
<h3>RNNGMMActorNetwork<a class="headerlink" href="#rnngmmactornetwork" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>An RNN policy network that outputs a multimodal Gaussian mixture distribution over actions from a sequence of observations.</p></li>
</ul>
</div>
<div class="section" id="vaeactor">
<h3>VAEActor<a class="headerlink" href="#vaeactor" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A VAE that models a distribution of actions conditioned on observations. The VAE prior and decoder are used at test-time as the policy.</p></li>
</ul>
</div>
</div>
<div class="section" id="implemented-value-networks">
<h2>Implemented Value Networks<a class="headerlink" href="#implemented-value-networks" title="Permalink to this headline">¶</a></h2>
<p>These networks take an observation dictionary as input (and possibly additional conditioning, such as subgoal or goal dictionaries) and produce value or action-value estimates or distributions. See <code class="docutils literal notranslate"><span class="pre">robomimic/models/value_nets.py</span></code> for complete implementations.</p>
<div class="section" id="valuenetwork">
<h3>ValueNetwork<a class="headerlink" href="#valuenetwork" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A basic value network that predicts values from observations. Can optionally be goal conditioned on future observations.</p></li>
</ul>
</div>
<div class="section" id="distributionalactionvaluenetwork">
<h3>DistributionalActionValueNetwork<a class="headerlink" href="#distributionalactionvaluenetwork" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Distributional Q (action-value) network that outputs a categorical distribution over a discrete grid of value atoms. See the [paper](https://arxiv.org/pdf/1707.06887.pdf for more details).</p></li>
</ul>
</div>
</div>
<div class="section" id="implemented-vaes">
<h2>Implemented VAEs<a class="headerlink" href="#implemented-vaes" title="Permalink to this headline">¶</a></h2>
<p>The library implements a general VAE architecture and a number of prior distributions. See <code class="docutils literal notranslate"><span class="pre">robomimic/models/vae_nets.py</span></code> for complete implementations.</p>
<div class="section" id="vae">
<h3>VAE<a class="headerlink" href="#vae" title="Permalink to this headline">¶</a></h3>
<p>A general Variational Autoencoder (VAE) implementation, as described in https://arxiv.org/abs/1312.6114.</p>
<p>Models a distribution p(X) or a conditional distribution p(X | Y), where each variable can consist of multiple modalities. The target variable X, whose distribution is modeled, is specified through the <code class="docutils literal notranslate"><span class="pre">input_shapes</span></code> argument, which is a map between modalities (strings) and expected shapes. In this way, a variable that consists of multiple kinds of data (e.g. image and flat-dimensional) can be modeled as well. A separate <code class="docutils literal notranslate"><span class="pre">output_shapes</span></code> argument is used to specify the expected reconstructions - this allows for asymmetric reconstruction (for example, reconstructing low-resolution images).</p>
<p>This implementation supports learning conditional distributions as well (cVAE). The conditioning variable Y is specified through the <code class="docutils literal notranslate"><span class="pre">condition_shapes</span></code> argument, which is also a map between modalities (strings) and expected shapes. In this way, variables with multiple kinds of data (e.g. image and flat-dimensional) can jointly be conditioned on. By default, the decoder takes the conditioning variable Y as input. To force the decoder to reconstruct from just the latent, set <code class="docutils literal notranslate"><span class="pre">decoder_is_conditioned</span></code> to False (in this case, the prior must be conditioned).</p>
<p>The implementation also supports learning expressive priors instead of using the usual N(0, 1) prior. There are three kinds of priors supported - Gaussian, Gaussian Mixture Model (GMM), and Categorical. For each prior, the parameters can be learned as independent parameters, or be learned as functions of the conditioning variable Y (by setting <code class="docutils literal notranslate"><span class="pre">prior_is_conditioned</span></code>).</p>
</div>
<div class="section" id="gaussianprior">
<h3>GaussianPrior<a class="headerlink" href="#gaussianprior" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A class that holds functionality for learning both unimodal Gaussian priors and multimodal Gaussian Mixture Model priors for use in VAEs. Supports learnable priors, learnable / fixed mixture weights for GMM, and observation-conditioned priors,</p></li>
</ul>
</div>
<div class="section" id="categoricalprior">
<h3>CategoricalPrior<a class="headerlink" href="#categoricalprior" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>A class that holds functionality for learning categorical priors for use in VAEs.</p></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="configs.html" class="btn btn-neutral float-right" title="Configs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="algorithms.html" class="btn btn-neutral float-left" title="Algorithms" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>